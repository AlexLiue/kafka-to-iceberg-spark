## Spark Configs


spark.master = local[2]
spark.app.name= Kafka2Iceberg

spark.sql.sources.partitionOverwriteMode = dynamic
spark.sql.extensions = org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.hive = org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.hive.type = hive
spark.hadoop.hive.metastore.uris = thrift://hadoop:9083
spark.sql.warehouse.dir = hdfs://hadoop:8020/user/hive/warehouse
spark.streaming.kafka.maxRatePerPartition=10000


# Kafka Configs
kafka.bootstrap.servers=kafka:9092
kafka.schema.registry.url=http://kafka:8081
kafka.consumer.group.id=g1-3
kafka.consumer.topic=test.db_gb18030_test.tbl_test
kafka.consumer.max.poll.records=5
kafka.consumer.key.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
kafka.consumer.value.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
kafka.consumer.commit.timeout.millis=60000

# Metadata Columns
record.metadata.source.columns = name, db, table, ts_ms, server_id, file, pos
record.metadata.source.prefix = _src_
record.metadata.transaction.columns = id, total_order, data_collection_order
record.metadata.transaction.prefix = _tsc_
record.metadata.kafka.columns = topic, partition, offset, timestamp
record.metadata.kafka.prefix = _kfk_

# Iceberg Configs
iceberg.table.name = hive.db_gb18030_test.tbl_test
iceberg.table.partitionBy = c1
iceberg.table.location = hdfs://hadoop:8020/user/hive/warehouse/db_gb18030_test.db/tbl_test
iceberg.table.comment = db_gb18030_test tbl_test
iceberg.table.properties = 'read.split.target-size'='268435456'

iceberg.table.primaryKey = c1,c2







