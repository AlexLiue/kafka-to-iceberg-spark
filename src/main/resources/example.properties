
spark.master = local[2]
spark.app.name= Kafka2Iceberg

kafka.bootstrap.servers=kafka:9092
kafka.schema.registry.url=http://kafka:8081
kafka.consumer.group.id=g1-3
kafka.consumer.topic=test.db_gb18030_test.tbl_test
kafka.consumer.max.poll.records=5
kafka.consumer.key.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
kafka.consumer.value.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer

kafka.consumer.commit.timeout.millis=60000

spark.streaming.kafka.maxRatePerPartition=5 
spark.streaming.kafka.consumer.cache.maxCapacity=0
spark.sql.sources.partitionOverwriteMode = dynamic
spark.sql.extensions = org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.hive = org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.hive.type = hive
spark.hadoop.hive.metastore.uris = thrift://hadoop:9083


iceberg.table.name = hive.db.tbl_test

record.metadata.source.columns = name, db, table, ts_ms, server_id, file, pos
record.metadata.source.prefix = _src_
record.metadata.transaction.columns = id, total_order, data_collection_order
record.metadata.transaction.prefix = _tsc_
record.metadata.kafka.columns = topic, partition, offset, timestamp
record.metadata.kafka.prefix = _kfk_
