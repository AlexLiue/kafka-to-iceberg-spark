#############################################################################################
#####                                   Spark Config                                    #####
#############################################################################################
spark.master = local[4]
spark.app.name= Kafka2Iceberg

spark.streaming.kafka.maxRatePerPartition = 10000


spark.yarn.jars = hdfs:/user/share/libs/spark/3.2.1/*.jar,hdfs:/user/share/libs/kafka2iceberg/0.1.0/*.jar
spark.sql.sources.partitionOverwriteMode = dynamic
spark.sql.extensions = org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

spark.sql.catalog.hadoop = org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.hadoop.type= hadoop
spark.sql.catalog.hadoop.warehouse = hdfs://hadoop:8020/user/test/iceberg

spark.sql.warehouse.dir = hdfs://hadoop:8020/user/hive/warehouse
spark.hadoop.hive.metastore.uris = thrift://hadoop:9083


#############################################################################################
#####                                 Hive JDBC Config                                  #####
#############################################################################################
hive.jdbc.url = jdbc:hive2://hadoop:10000
hive.jdbc.user =
hive.jdbc.password =
hive.external.jar = hdfs://hadoop:8020/user/share/libs/iceberg-runtime/0.13.2/iceberg-hive-runtime-0.13.2.jar

#############################################################################################
#####                                Kafka Configs                                      #####
#############################################################################################
kafka.bootstrap.servers=kafka:9092
kafka.schema.registry.url=http://kafka:8081
kafka.consumer.group.id=iceberg-hadoop
kafka.consumer.topic=test.db_gb18030_test.tbl_test_1
kafka.consumer.max.poll.records=5
kafka.consumer.key.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
kafka.consumer.value.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
kafka.consumer.commit.timeout.millis=60000

# Metadata Columns
record.metadata.source.columns = name, db, table, ts_ms, server_id, file, pos
record.metadata.source.prefix = _src_
record.metadata.transaction.columns = id, total_order, data_collection_order
record.metadata.transaction.prefix = _tsc_
record.metadata.kafka.columns = topic, partition, offset, timestamp
record.metadata.kafka.prefix = _kfk_

#############################################################################################
#####                                Iceberg Configs                                    #####
#############################################################################################
iceberg.table.name = hadoop.db_gb18030_test.tbl_test_1
iceberg.table.partitionBy = days(_src_time)
iceberg.table.comment = db_gb18030_test tbl_test_1
iceberg.table.properties = 'write.metadata.delete-after-commit.enabled'='true'
iceberg.table.primaryKey = id


iceberg.maintenance.enabled = true
iceberg.maintenance.triggering.time = 15:00:00
iceberg.maintenance.triggering.interval = 86400000
iceberg.maintenance.snapshot.expire.time = 86400000

iceberg.maintenance.compact.filter.column = _src_ts_ms
iceberg.maintenance.compact.day.offset = 0
iceberg.maintenance.compact.target.file.size.bytes = 134217728
iceberg.maintenance.manifests.file.length = 10485760


#iceberg.maintenance.enabled = true
#iceberg.maintenance.triggering.time = 21:00:00
#iceberg.maintenance.triggering.interval = 86400000
#iceberg.maintenance.snapshot.expire.time = 86400000
#
#iceberg.maintenance.compact.filter.column = _src_ts_ms
#iceberg.maintenance.compact.day.offset = 0
#iceberg.maintenance.compact.target.file.size.bytes = 134217728
#iceberg.maintenance.manifests.file.length = 10485760





