## Spark Configs

spark.master = yarn
spark.app.name= Kafka2Iceberg

spark.yarn.jars = hdfs:/user/share/libs/spark/3.2.1/*.jar,hdfs:/user/share/libs/kafka2iceberg/0.1.0/*.jar
spark.sql.sources.partitionOverwriteMode = dynamic
spark.sql.extensions = org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.hadoop = org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.hadoop.type= hadoop
spark.sql.catalog.hadoop.warehouse = hdfs://hadoop:8020/user/test/iceberg
spark.streaming.kafka.maxRatePerPartition=10000


# Kafka Configs
kafka.bootstrap.servers=kafka:9092
kafka.schema.registry.url=http://kafka:8081
kafka.consumer.group.id=iceberg-hadoop
kafka.consumer.topic=test.db_gb18030_test.tbl_test_1
kafka.consumer.max.poll.records=5
kafka.consumer.key.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
kafka.consumer.value.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
kafka.consumer.commit.timeout.millis=60000

# Metadata Columns
record.metadata.source.columns = name, db, table, ts_ms, server_id, file, pos
record.metadata.source.prefix = _src_
record.metadata.transaction.columns = id, total_order, data_collection_order
record.metadata.transaction.prefix = _tsc_
record.metadata.kafka.columns = topic, partition, offset, timestamp
record.metadata.kafka.prefix = _kfk_

# Iceberg Configs
iceberg.table.name = hadoop.db_gb18030_test.tbl_test_1
iceberg.table.partitionBy = id1,id2
iceberg.table.comment = db_gb18030_test tbl_test_1
iceberg.table.properties = 'read.split.target-size'='268435456'
iceberg.table.primaryKey = id1,id2